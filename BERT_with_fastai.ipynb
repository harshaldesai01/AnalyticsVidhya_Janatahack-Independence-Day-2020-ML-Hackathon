{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BERT with fastai.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ-DTLjrYFs9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "094ee78e-9726-4c33-8589-59b009cbbf38"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f_vFPAiL0Llm",
        "colab": {}
      },
      "source": [
        "from fastai import *\n",
        "from fastai.text import *"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9dJiDMkPHfbE",
        "colab": {}
      },
      "source": [
        "!unzip -qq '/content/drive/My Drive/Analytics Vidhya/input/train.zip'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y7pbQjsaHjrE",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv('/content/train.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B8h_BnQiHl6J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "a5f7ce53-2315-4174-9ecb-5278877eeed3"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>TITLE</th>\n",
              "      <th>ABSTRACT</th>\n",
              "      <th>Computer Science</th>\n",
              "      <th>Physics</th>\n",
              "      <th>Mathematics</th>\n",
              "      <th>Statistics</th>\n",
              "      <th>Quantitative Biology</th>\n",
              "      <th>Quantitative Finance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Reconstructing Subject-Specific Effect Maps</td>\n",
              "      <td>Predictive models allow subject-specific inf...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Rotation Invariance Neural Network</td>\n",
              "      <td>Rotation invariance and translation invarian...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
              "      <td>We introduce and develop the notion of spher...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>A finite element approximation for the stochas...</td>\n",
              "      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
              "      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID  ... Quantitative Finance\n",
              "0   1  ...                    0\n",
              "1   2  ...                    0\n",
              "2   3  ...                    0\n",
              "3   4  ...                    0\n",
              "4   5  ...                    0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wORUjN41Hn30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "8d94f064-8c63-4a96-bd65-1092f365105f"
      },
      "source": [
        "train_df.info()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20972 entries, 0 to 20971\n",
            "Data columns (total 9 columns):\n",
            " #   Column                Non-Null Count  Dtype \n",
            "---  ------                --------------  ----- \n",
            " 0   ID                    20972 non-null  int64 \n",
            " 1   TITLE                 20972 non-null  object\n",
            " 2   ABSTRACT              20972 non-null  object\n",
            " 3   Computer Science      20972 non-null  int64 \n",
            " 4   Physics               20972 non-null  int64 \n",
            " 5   Mathematics           20972 non-null  int64 \n",
            " 6   Statistics            20972 non-null  int64 \n",
            " 7   Quantitative Biology  20972 non-null  int64 \n",
            " 8   Quantitative Finance  20972 non-null  int64 \n",
            "dtypes: int64(7), object(2)\n",
            "memory usage: 1.4+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ySeo_q7eHqVX",
        "colab": {}
      },
      "source": [
        "train_df['text'] = train_df['TITLE'] + train_df['ABSTRACT']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vFzJlLdoHxNe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "770195cb-b24b-4628-9631-3e20bb853a99"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>TITLE</th>\n",
              "      <th>ABSTRACT</th>\n",
              "      <th>Computer Science</th>\n",
              "      <th>Physics</th>\n",
              "      <th>Mathematics</th>\n",
              "      <th>Statistics</th>\n",
              "      <th>Quantitative Biology</th>\n",
              "      <th>Quantitative Finance</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Reconstructing Subject-Specific Effect Maps</td>\n",
              "      <td>Predictive models allow subject-specific inf...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Reconstructing Subject-Specific Effect Maps  P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Rotation Invariance Neural Network</td>\n",
              "      <td>Rotation invariance and translation invarian...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Rotation Invariance Neural Network  Rotation i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
              "      <td>We introduce and develop the notion of spher...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Spherical polyharmonics and Poisson kernels fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>A finite element approximation for the stochas...</td>\n",
              "      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>A finite element approximation for the stochas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
              "      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Comparative study of Discrete Wavelet Transfor...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID  ...                                               text\n",
              "0   1  ...  Reconstructing Subject-Specific Effect Maps  P...\n",
              "1   2  ...  Rotation Invariance Neural Network  Rotation i...\n",
              "2   3  ...  Spherical polyharmonics and Poisson kernels fo...\n",
              "3   4  ...  A finite element approximation for the stochas...\n",
              "4   5  ...  Comparative study of Discrete Wavelet Transfor...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BAWbaeVGHzJt",
        "colab": {}
      },
      "source": [
        "!unzip -qq '/content/drive/My Drive/Analytics Vidhya/input/test.zip'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cptie0mEH3qu",
        "colab": {}
      },
      "source": [
        "test_df = pd.read_csv('/content/test.csv')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5BPen0vVH6JD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "cc30f1ac-e57a-4594-8952-fa0027b3093a"
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>TITLE</th>\n",
              "      <th>ABSTRACT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20973</td>\n",
              "      <td>Closed-form Marginal Likelihood in Gamma-Poiss...</td>\n",
              "      <td>We present novel understandings of the Gamma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20974</td>\n",
              "      <td>Laboratory mid-IR spectra of equilibrated and ...</td>\n",
              "      <td>Meteorites contain minerals from Solar Syste...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20975</td>\n",
              "      <td>Case For Static AMSDU Aggregation in WLANs</td>\n",
              "      <td>Frame aggregation is a mechanism by which mu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20976</td>\n",
              "      <td>The $Gaia$-ESO Survey: the inner disk intermed...</td>\n",
              "      <td>Milky Way open clusters are very diverse in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20977</td>\n",
              "      <td>Witness-Functions versus Interpretation-Functi...</td>\n",
              "      <td>Proving that a cryptographic protocol is cor...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      ID  ...                                           ABSTRACT\n",
              "0  20973  ...    We present novel understandings of the Gamma...\n",
              "1  20974  ...    Meteorites contain minerals from Solar Syste...\n",
              "2  20975  ...    Frame aggregation is a mechanism by which mu...\n",
              "3  20976  ...    Milky Way open clusters are very diverse in ...\n",
              "4  20977  ...    Proving that a cryptographic protocol is cor...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2JCemixnH8JS",
        "colab": {}
      },
      "source": [
        "test_id = test_df['ID']"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pMM71VesH_Oq",
        "colab": {}
      },
      "source": [
        "test_df['text'] = test_df['TITLE'] + test_df['ABSTRACT']"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O8PgaurJIGfx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "f4125a17-5e67-46c5-8e86-576933322d58"
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>TITLE</th>\n",
              "      <th>ABSTRACT</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20973</td>\n",
              "      <td>Closed-form Marginal Likelihood in Gamma-Poiss...</td>\n",
              "      <td>We present novel understandings of the Gamma...</td>\n",
              "      <td>Closed-form Marginal Likelihood in Gamma-Poiss...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20974</td>\n",
              "      <td>Laboratory mid-IR spectra of equilibrated and ...</td>\n",
              "      <td>Meteorites contain minerals from Solar Syste...</td>\n",
              "      <td>Laboratory mid-IR spectra of equilibrated and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20975</td>\n",
              "      <td>Case For Static AMSDU Aggregation in WLANs</td>\n",
              "      <td>Frame aggregation is a mechanism by which mu...</td>\n",
              "      <td>Case For Static AMSDU Aggregation in WLANs  Fr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20976</td>\n",
              "      <td>The $Gaia$-ESO Survey: the inner disk intermed...</td>\n",
              "      <td>Milky Way open clusters are very diverse in ...</td>\n",
              "      <td>The $Gaia$-ESO Survey: the inner disk intermed...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20977</td>\n",
              "      <td>Witness-Functions versus Interpretation-Functi...</td>\n",
              "      <td>Proving that a cryptographic protocol is cor...</td>\n",
              "      <td>Witness-Functions versus Interpretation-Functi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      ID  ...                                               text\n",
              "0  20973  ...  Closed-form Marginal Likelihood in Gamma-Poiss...\n",
              "1  20974  ...  Laboratory mid-IR spectra of equilibrated and ...\n",
              "2  20975  ...  Case For Static AMSDU Aggregation in WLANs  Fr...\n",
              "3  20976  ...  The $Gaia$-ESO Survey: the inner disk intermed...\n",
              "4  20977  ...  Witness-Functions versus Interpretation-Functi...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DrTokfDBIc7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "fd54202b-98f3-46a2-99aa-2c6afc68bf98"
      },
      "source": [
        "%%bash\n",
        "pip install pytorch-pretrained-bert"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.14.37)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.37 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.17.37)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.16.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2020.6.20)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.37->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.37->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.37->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jhpCVlKFIH-I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cab6d30f-e925-410b-899c-43f38f71c967"
      },
      "source": [
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "bert_tok = BertTokenizer.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 891100.99B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VPF9ly7JINuC",
        "colab": {}
      },
      "source": [
        "class FastAiBertTokenizer(BaseTokenizer):\n",
        "    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n",
        "    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=128, **kwargs):\n",
        "        self._pretrained_tokenizer = tokenizer\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self\n",
        "\n",
        "    def tokenizer(self, t:str) -> List[str]:\n",
        "        \"\"\"Limits the maximum sequence length\"\"\"\n",
        "        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SNvDKRibItN5",
        "colab": {}
      },
      "source": [
        "fastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZkhcIjBJI0mY",
        "colab": {}
      },
      "source": [
        "fastai_tokenizer = Tokenizer(tok_func=FastAiBertTokenizer(bert_tok, max_seq_len=256), pre_rules=[], post_rules=[])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q1fQYtCdI4DI",
        "colab": {}
      },
      "source": [
        "label_cols = ['Computer Science', 'Physics' , 'Mathematics' , 'Statistics' , 'Quantitative Biology' , 'Quantitative Finance']"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tonu2UrgKF55",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yQC2BAH2KHzz",
        "colab": {}
      },
      "source": [
        "train, val = train_test_split(train_df, shuffle=True, test_size=0.1, random_state=42)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WoazY7yDKOzo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "929b4d0d-39c1-45fd-c69f-593d1fa3a209"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>TITLE</th>\n",
              "      <th>ABSTRACT</th>\n",
              "      <th>Computer Science</th>\n",
              "      <th>Physics</th>\n",
              "      <th>Mathematics</th>\n",
              "      <th>Statistics</th>\n",
              "      <th>Quantitative Biology</th>\n",
              "      <th>Quantitative Finance</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13275</th>\n",
              "      <td>13276</td>\n",
              "      <td>Clustering in Hilbert space of a quantum optim...</td>\n",
              "      <td>The solution space of many classical optimiz...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Clustering in Hilbert space of a quantum optim...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19273</th>\n",
              "      <td>19274</td>\n",
              "      <td>Graph heat mixture model learning</td>\n",
              "      <td>Graph inference methods have recently attrac...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Graph heat mixture model learning  Graph infer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6427</th>\n",
              "      <td>6428</td>\n",
              "      <td>Fast and unsupervised methods for multilingual...</td>\n",
              "      <td>In this paper we explore the use of unsuperv...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Fast and unsupervised methods for multilingual...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19168</th>\n",
              "      <td>19169</td>\n",
              "      <td>Natasha: Faster Non-Convex Stochastic Optimiza...</td>\n",
              "      <td>Given a nonconvex function that is an averag...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Natasha: Faster Non-Convex Stochastic Optimiza...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14148</th>\n",
              "      <td>14149</td>\n",
              "      <td>Kustaanheimo-Stiefel transformation with an ar...</td>\n",
              "      <td>Kustaanheimo-Stiefel (KS) transformation dep...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Kustaanheimo-Stiefel transformation with an ar...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          ID  ...                                               text\n",
              "13275  13276  ...  Clustering in Hilbert space of a quantum optim...\n",
              "19273  19274  ...  Graph heat mixture model learning  Graph infer...\n",
              "6427    6428  ...  Fast and unsupervised methods for multilingual...\n",
              "19168  19169  ...  Natasha: Faster Non-Convex Stochastic Optimiza...\n",
              "14148  14149  ...  Kustaanheimo-Stiefel transformation with an ar...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "05QfYnjaKSZ5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "90e9d2ab-4462-4a99-9882-e674e081a05c"
      },
      "source": [
        "val.head()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>TITLE</th>\n",
              "      <th>ABSTRACT</th>\n",
              "      <th>Computer Science</th>\n",
              "      <th>Physics</th>\n",
              "      <th>Mathematics</th>\n",
              "      <th>Statistics</th>\n",
              "      <th>Quantitative Biology</th>\n",
              "      <th>Quantitative Finance</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20257</th>\n",
              "      <td>20258</td>\n",
              "      <td>Dynamic Layer Normalization for Adaptive Neura...</td>\n",
              "      <td>Layer normalization is a recently introduced...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Dynamic Layer Normalization for Adaptive Neura...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>482</th>\n",
              "      <td>483</td>\n",
              "      <td>Susceptibility Propagation by Using Diagonal C...</td>\n",
              "      <td>A susceptibility propagation that is constru...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Susceptibility Propagation by Using Diagonal C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4189</th>\n",
              "      <td>4190</td>\n",
              "      <td>The Robot Routing Problem for Collecting Aggre...</td>\n",
              "      <td>We propose a new model for formalizing rewar...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>The Robot Routing Problem for Collecting Aggre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9838</th>\n",
              "      <td>9839</td>\n",
              "      <td>Probability, Statistics and Planet Earth, I: G...</td>\n",
              "      <td>The study of covariances (or positive defini...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Probability, Statistics and Planet Earth, I: G...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16591</th>\n",
              "      <td>16592</td>\n",
              "      <td>Counting the number of metastable states in th...</td>\n",
              "      <td>Modularity maximization using greedy algorit...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Counting the number of metastable states in th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          ID  ...                                               text\n",
              "20257  20258  ...  Dynamic Layer Normalization for Adaptive Neura...\n",
              "482      483  ...  Susceptibility Propagation by Using Diagonal C...\n",
              "4189    4190  ...  The Robot Routing Problem for Collecting Aggre...\n",
              "9838    9839  ...  Probability, Statistics and Planet Earth, I: G...\n",
              "16591  16592  ...  Counting the number of metastable states in th...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pynqbOUVJD3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "035910d4-f60b-4f97-ec05-673fda8bb41e"
      },
      "source": [
        "databunch_1 = TextDataBunch.from_df(\".\", train, val, \n",
        "                  tokenizer=fastai_tokenizer,\n",
        "                  vocab=fastai_bert_vocab,\n",
        "                  include_bos=False,\n",
        "                  include_eos=False,\n",
        "                  text_cols=\"text\",\n",
        "                  label_cols=label_cols,\n",
        "                  bs=16,\n",
        "                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n",
        "             )"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SQMjJiFDKc0s",
        "colab": {}
      },
      "source": [
        "class BertTokenizeProcessor(TokenizeProcessor):\n",
        "    def __init__(self, tokenizer):\n",
        "        super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n",
        "\n",
        "class BertNumericalizeProcessor(NumericalizeProcessor):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, vocab=Vocab(list(bert_tok.vocab.keys())), **kwargs)\n",
        "\n",
        "def get_bert_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n",
        "    \"\"\"\n",
        "    Constructing preprocessors for BERT\n",
        "    We remove sos/eos tokens since we add that ourselves in the tokenizer.\n",
        "    We also use a custom vocabulary to match the numericalization with the original BERT model.\n",
        "    \"\"\"\n",
        "    return [BertTokenizeProcessor(tokenizer=tokenizer),\n",
        "            NumericalizeProcessor(vocab=vocab)]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ajl4R7cOKlGO",
        "colab": {}
      },
      "source": [
        "class BertDataBunch(TextDataBunch):\n",
        "    @classmethod\n",
        "    def from_df(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n",
        "                tokenizer:Tokenizer=None, vocab:Vocab=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n",
        "                label_cols:IntsOrStrs=0, label_delim:str=None, **kwargs) -> DataBunch:\n",
        "        \"Create a `TextDataBunch` from DataFrames.\"\n",
        "        p_kwargs, kwargs = split_kwargs_by_func(kwargs, get_bert_processor)\n",
        "        # use our custom processors while taking tokenizer and vocab as kwargs\n",
        "        processor = get_bert_processor(tokenizer=tokenizer, vocab=vocab, **p_kwargs)\n",
        "        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n",
        "        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n",
        "                        TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n",
        "        src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_df(cols=label_cols, classes=classes)\n",
        "        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n",
        "        return src.databunch(**kwargs)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4GpS8HfSK40B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "443f0f65-535e-45fe-895a-9e0f3cf6653f"
      },
      "source": [
        "databunch_1.show_batch()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/fastai/text/data.py:339: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero()\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  idx_min = (t != self.pad_idx).nonzero().min()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>[CLS] the landscape of ne ##uro ##ima ##ge - ing research as the field of ne ##uro ##ima ##ging grows , it can be difficult for scientists within the field to gain and maintain a detailed understanding of its ever - changing landscape . while collaboration and citation networks highlight important contributions within the field , the roles of and relations among specific areas of study can remain quite opaque</td>\n",
              "      <td>Computer Science;Statistics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[CLS] the spectrum , radiation conditions and the fred ##holm property for the dir ##ich ##let lap ##la ##cian in a per ##for ##ated plane with semi - infinite inclusion ##s we consider the spectral dir ##ich ##let problem for the lap ##lace operator in the plane $ \\ omega ^ { \\ ci ##rc } $ with double - periodic per ##for ##ation but also in the domain $</td>\n",
              "      <td>Mathematics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[CLS] effects of pressure and magnetic field on the re - en ##tra ##nt super ##con ##du ##ctor eu ( fe $ _ { 0 . 93 } $ r ##h $ _ { 0 . 07 } $ ) $ _ 2 $ as $ _ 2 $ electron - do ##ped eu ( fe $ _ { 0 . 93 } $ r ##h $ _ { 0</td>\n",
              "      <td>Physics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[CLS] mis ##com ##put ##ation in software : learning to live with errors computer programs do not always work as expected . in fact , ominous warnings about the desperate state of the software industry continue to be released with almost ritual ##istic regular ##ity . in this paper , we look at the 60 years history of programming and at the different practical methods that software community developed to</td>\n",
              "      <td>Computer Science</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>[CLS] does a generalized cha ##ply ##gin gas correctly describe the co ##smo ##logical dark sector ? yes , but only for a parameter value that makes it almost coincide with the standard model . we rec ##ons ##ider the co ##smo ##logical dynamics of a generalized cha ##ply ##gin gas ( g ##c ##g ) which is split into a cold dark matter ( cd ##m ) part and</td>\n",
              "      <td>Physics</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5xbSPdtJK8DV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "30b376f5-30e4-4520-8724-bd47ab8cffc3"
      },
      "source": [
        "from pytorch_pretrained_bert.modeling import BertConfig, BertForSequenceClassification, BertForNextSentencePrediction, BertForMaskedLM\n",
        "bert_model_class = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:11<00:00, 34720219.01B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RXfkrguoLb1m",
        "colab": {}
      },
      "source": [
        "loss_func = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DNJvHlnSLfHR",
        "colab": {}
      },
      "source": [
        "acc_02 = partial(accuracy_thresh, thresh=0.2)\n",
        "f1_score = partial(fbeta, thresh=0.2, beta=1)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LI9hnZkZLnk2",
        "colab": {}
      },
      "source": [
        "model = bert_model_class"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RxEW0qoRLrnO",
        "colab": {}
      },
      "source": [
        "from fastai.callbacks import *\n",
        "\n",
        "learner = Learner(\n",
        "    databunch_1, model,\n",
        "    loss_func=loss_func, model_dir='/content/drive/My Drive/Analytics Vidhya/models/bert', metrics=[acc_02, f1_score],\n",
        ")"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iXZtyu9xMR7Q",
        "colab": {}
      },
      "source": [
        "def bert_clas_split(self) -> List[nn.Module]:\n",
        "    \n",
        "    bert = model.bert\n",
        "    embedder = bert.embeddings\n",
        "    pooler = bert.pooler\n",
        "    encoder = bert.encoder\n",
        "    classifier = [model.dropout, model.classifier]\n",
        "    n = len(encoder.layer)//3\n",
        "    print(n)\n",
        "    groups = [[embedder], list(encoder.layer[:n]), list(encoder.layer[n+1:2*n]), list(encoder.layer[(2*n)+1:]), [pooler], classifier]\n",
        "    return groups"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JN8y8tUXMX4q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "957438e6-1ff8-400f-e699-10ef471f7bf7"
      },
      "source": [
        "x = bert_clas_split(model)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hkZlLbfbMajr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a5328504-1152-44ca-e0c9-3599b4d8f2ec"
      },
      "source": [
        "learner.split([x[0], x[1], x[2], x[3], x[5]])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Learner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (18874 items)\n",
              "x: TextList\n",
              "[CLS] cluster ##ing in hilbert space of a quantum optimization problem the solution space of many classical optimization problems breaks up into clusters which are extensively distant from one another in the ham ##ming metric . here , we show that an analogous quantum cluster ##ing phenomenon takes place in the ground state subsp ##ace of a certain quantum optimization problem . this involves extending the notion of cluster ##ing to hilbert space , where the classical ham ##ming distance is not immediately useful . quantum clusters correspond to macro ##scopic ##ally distinct subsp ##ace ##s of the full quantum ground state space which grow with the system size . we explicitly demonstrate that such clusters arise in the solution space of random quantum sat ##is ##fia ##bility ( 3 - q ##sat ) at its sat ##is ##fia ##bility transition . we estimate both the number of these clusters and their internal entropy . the former are given by the number of hardcore dime ##r covering ##s of the core of the interaction graph , while the latter is related to the under ##con ##stra ##ined degrees of freedom not touched by the dime ##rs . we additionally provide new numerical evidence suggesting that the 3 - q ##sat sat ##is ##fia ##bility transition may coincide with the product sat ##is ##fia ##bility transition , which would imply the absence of an intermediate en ##tangled sat ##is ##fia ##ble phase . [SEP],[CLS] graph heat mixture model learning graph inference methods have recently attracted a great interest from the scientific community , due to the large value they bring in data interpretation and analysis . however , most of the available state - of - the - art methods focus on scenarios where all available data can be explained through the same graph , or groups corresponding to each graph are known a prior ##i . in this paper , we argue that this is not always realistic and we introduce a genera ##tive model for mixed signals following a heat diffusion process on multiple graphs . we propose an expectation - maxim ##isation algorithm that can successfully separate signals into corresponding groups , and in ##fer multiple graphs that govern their behaviour . we demonstrate the benefits of our method on both synthetic and real data . [SEP],[CLS] fast and un ##su ##per ##vis ##ed methods for multi ##ling ##ual co ##gna ##te cluster ##ing in this paper we explore the use of un ##su ##per ##vis ##ed methods for detecting co ##gna ##tes in multi ##ling ##ual word lists . we use online em to train sound segment similarity weights for computing similarity between two words . we tested our online systems on geographically spread sixteen different language groups of the world and show that the online pm ##i system ( point ##wise mutual information ) out ##per ##forms a hmm based system and two linguistic ##ally motivated systems : lex ##sta ##t and ali ##ne . our results suggest that a pm ##i system trained in an online fashion can be used by historical linguist ##s for fast and accurate identification of co ##gna ##tes in not so well - studied language families . [SEP],[CLS] natasha : faster non - convex st ##och ##astic optimization via strongly non - convex parameter given a non ##con ##ve ##x function that is an average of $ n $ smooth functions , we design st ##och ##astic first - order methods to find its approximate stationary points . the convergence of our new methods depends on the smallest ( negative ) e ##igen ##val ##ue $ - \\ sigma $ of the hess ##ian , a parameter that describes how non ##con ##ve ##x the function is . our methods out ##per ##form known results for a range of parameter $ \\ sigma $ , and can be used to find approximate local mini ##ma . our result implies an interesting di ##cho ##tom ##y : there exists a threshold $ \\ sigma _ 0 $ so that the currently fastest methods for $ \\ sigma > \\ sigma _ 0 $ and for $ \\ sigma < \\ sigma _ 0 $ have different behaviors : the former scales with $ n ^ { 2 / 3 } $ and the latter scales with $ n ^ { 3 / 4 } $ . [SEP],[CLS] ku ##sta ##an ##heim ##o - st ##ie ##fe ##l transformation with an arbitrary defining vector ku ##sta ##an ##heim ##o - st ##ie ##fe ##l ( ks ) transformation depends on the choice of some preferred direction in the cart ##esian 3d space . this choice , seldom explicitly mentioned , amounts typically to the direction of the first or the third coordinate axis in celestial mechanics and atomic physics , respectively . the present work develops a canonical ks transformation with an arbitrary preferred direction , indicated by what we call a defining vector . using a mix of vector and qu ##ater ##nio ##n algebra , we formula ##te the transformation in a reference frame independent manner . the link between the os ##ci ##lla ##tor and kepler ##ian first integral ##s is given . as an example of the present formulation , the kepler ##ian motion in a rotating frame is re - investigated . [SEP]\n",
              "y: MultiCategoryList\n",
              "Computer Science;Physics,Computer Science;Statistics,Computer Science,Computer Science;Mathematics;Statistics,Mathematics\n",
              "Path: .;\n",
              "\n",
              "Valid: LabelList (2098 items)\n",
              "x: TextList\n",
              "[CLS] dynamic layer normal ##ization for adaptive neural acoustic modeling in speech recognition layer normal ##ization is a recently introduced technique for normal ##izing the activities of neurons in deep neural networks to improve the training speed and stability . in this paper , we introduce a new layer normal ##ization technique called dynamic layer normal ##ization ( dl ##n ) for adaptive neural acoustic modeling in speech recognition . by dynamic ##ally generating the scaling and shifting parameters in layer normal ##ization , dl ##n adapt ##s neural acoustic models to the acoustic variability arising from various factors such as speakers , channel noises , and environments . unlike other adaptive acoustic models , our proposed approach does not require additional adaptation data or speaker information such as i - vectors . moreover , the model size is fixed as it dynamic ##ally generates adaptation parameters . we apply our proposed dl ##n to deep bid ##ire ##ction ##al l ##st ##m acoustic models and evaluate them on two bench ##mark data ##set ##s for large vocabulary as ##r experiments : w ##s ##j and ted - liu ##m release 2 . the experimental results show that our dl ##n improves neural acoustic models in terms of transcription accuracy by dynamic ##ally adapting to various speakers and environments . [SEP],[CLS] su ##sc ##ept ##ibility propagation by using diagonal consistency a su ##sc ##ept ##ibility propagation that is constructed by combining a belief propagation and a linear response method is used for approximate computation for marko ##v random fields . here ##in , we formula ##te a new , improved su ##sc ##ept ##ibility propagation by using the concept of a diagonal matching method that is based on mean - field approaches to inverse is ##ing problems . the proposed su ##sc ##ept ##ibility propagation is robust for various network structures , and it is reduced to the ordinary su ##sc ##ept ##ibility propagation and to the adaptive thou ##less - anderson - palmer equation in special cases . [SEP],[CLS] the robot routing problem for collecting aggregate st ##och ##astic rewards we propose a new model for formal ##izing reward collection problems on graphs with dynamic ##ally generated rewards which may appear and disappear based on a st ##och ##astic model . the * robot routing problem * is modeled as a graph whose nodes are st ##och ##astic processes generating potential rewards over discrete time . the rewards are generated according to the st ##och ##astic process , but at each step , an existing reward disappears with a given probability . the edges in the graph en ##code the ( unit - distance ) paths between the rewards ' locations . on visiting a node , the robot collects the accumulated reward at the node at that time , but traveling between the nodes takes time . the optimization question asks to compute an optimal ( or epsilon - optimal ) path that maximize ##s the expected collected rewards . we consider the finite and infinite - horizon robot routing problems . for finite - horizon , the goal is to maximize the total expected reward , while for infinite horizon we consider limit - average objectives . we study the computational and strategy complexity of these problems , establish np - lower bounds and show that optimal strategies require memory in general . we also provide an algorithm for computing epsilon - optimal infinite paths for arbitrary epsilon > 0 . [SEP],[CLS] probability , statistics and planet earth , i : geo ##tem ##por ##al co ##var ##iance ##s the study of co ##var ##iance ##s ( or positive definite functions ) on the sphere ( the earth , in our motivation ) goes back to bo ##chner and sc ##hoe ##nberg ( 1940 - - 42 ) and to the first author ( 1969 , 1973 ) , among others . extending to the geo ##tem ##por ##al case ( sphere cross line , for position and time ) was for a long time an obstacle to geo ##sta ##tist ##ical modelling . the character ##isation question here was raised by the authors and mi ##ja ##tov ##ic in 2016 , and answered by berg and por ##cu in 2017 . extensions to multiple products ( of spheres and lines ) follows similarly ( gu ##ella , men ##ega ##tto and per ##on , 2016 ) . we survey results of this type , and related applications e . g . in numerical weather prediction . [SEP],[CLS] counting the number of meta ##sta ##ble states in the modular ##ity landscape : algorithm ##ic detect ##ability limit of greedy algorithms in community detection modular ##ity maxim ##ization using greedy algorithms continues to be a popular approach toward community detection in graphs , even after various better forming algorithms have been proposed . apart from its clear mechanism and ease of implementation , this approach is persistent ##ly popular because , presumably , its risk of algorithm ##ic failure is not well understood . this rapid communication provides insight into this issue by est ##imating the algorithm ##ic performance limit of modular ##ity maxim ##ization . this is achieved by counting the number of meta ##sta ##ble states under a local update rule . our results offer a quantitative insight into the level of spa ##rs ##ity at which a greedy algorithm typically fails . [SEP]\n",
              "y: MultiCategoryList\n",
              "Computer Science,Mathematics;Statistics,Computer Science;Mathematics,Physics;Statistics,Computer Science\n",
              "Path: .;\n",
              "\n",
              "Test: None, model=BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=BCEWithLogitsLoss(), metrics=[functools.partial(<function accuracy_thresh at 0x7f59122ffd90>, thresh=0.2), functools.partial(<function fbeta at 0x7f59122ff9d8>, thresh=0.2, beta=1)], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='/content/drive/My Drive/Analytics Vidhya/models/bert', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[Sequential(\n",
              "  (0): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): BertLayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (1): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (2): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (3): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (1): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (2): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (1): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (2): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): Dropout(p=0.1, inplace=False)\n",
              "  (1): Linear(in_features=768, out_features=6, bias=True)\n",
              ")], add_time=True, silent=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RnmRIzO8Md6U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "d280ed11-8253-4a95-94ae-0b0475c071a3"
      },
      "source": [
        "learner.lr_find()\n",
        "learner.recorder.plot()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.00% [0/1 00:00<00:00]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy_thresh</th>\n",
              "      <th>fbeta</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='83' class='' max='1179' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      7.04% [83/1179 00:33<07:28 1.9605]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-BMsw-UgMllm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "260c1e80-9deb-49ea-e221-b4c41ee3d3c8"
      },
      "source": [
        "learner.fit_one_cycle(3, max_lr=slice(1e-5, 5e-4), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy_thresh</th>\n",
              "      <th>fbeta</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.198767</td>\n",
              "      <td>0.193926</td>\n",
              "      <td>0.901335</td>\n",
              "      <td>0.821174</td>\n",
              "      <td>08:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.160616</td>\n",
              "      <td>0.168523</td>\n",
              "      <td>0.912456</td>\n",
              "      <td>0.839271</td>\n",
              "      <td>08:16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.128783</td>\n",
              "      <td>0.168149</td>\n",
              "      <td>0.919844</td>\n",
              "      <td>0.849857</td>\n",
              "      <td>08:30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wpi4i9wjSPVw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3308b90d-e5d3-4697-be4d-c04a7acef265"
      },
      "source": [
        "learner.save('head')\n",
        "learner.load('head')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Learner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (18874 items)\n",
              "x: TextList\n",
              "[CLS] cluster ##ing in hilbert space of a quantum optimization problem the solution space of many classical optimization problems breaks up into clusters which are extensively distant from one another in the ham ##ming metric . here , we show that an analogous quantum cluster ##ing phenomenon takes place in the ground state subsp ##ace of a certain quantum optimization problem . this involves extending the notion of cluster ##ing to hilbert space , where the classical ham ##ming distance is not immediately useful . quantum clusters correspond to macro ##scopic ##ally distinct subsp ##ace ##s of the full quantum ground state space which grow with the system size . we explicitly demonstrate that such clusters arise in the solution space of random quantum sat ##is ##fia ##bility ( 3 - q ##sat ) at its sat ##is ##fia ##bility transition . we estimate both the number of these clusters and their internal entropy . the former are given by the number of hardcore dime ##r covering ##s of the core of the interaction graph , while the latter is related to the under ##con ##stra ##ined degrees of freedom not touched by the dime ##rs . we additionally provide new numerical evidence suggesting that the 3 - q ##sat sat ##is ##fia ##bility transition may coincide with the product sat ##is ##fia ##bility transition , which would imply the absence of an intermediate en ##tangled sat ##is ##fia ##ble phase . [SEP],[CLS] graph heat mixture model learning graph inference methods have recently attracted a great interest from the scientific community , due to the large value they bring in data interpretation and analysis . however , most of the available state - of - the - art methods focus on scenarios where all available data can be explained through the same graph , or groups corresponding to each graph are known a prior ##i . in this paper , we argue that this is not always realistic and we introduce a genera ##tive model for mixed signals following a heat diffusion process on multiple graphs . we propose an expectation - maxim ##isation algorithm that can successfully separate signals into corresponding groups , and in ##fer multiple graphs that govern their behaviour . we demonstrate the benefits of our method on both synthetic and real data . [SEP],[CLS] fast and un ##su ##per ##vis ##ed methods for multi ##ling ##ual co ##gna ##te cluster ##ing in this paper we explore the use of un ##su ##per ##vis ##ed methods for detecting co ##gna ##tes in multi ##ling ##ual word lists . we use online em to train sound segment similarity weights for computing similarity between two words . we tested our online systems on geographically spread sixteen different language groups of the world and show that the online pm ##i system ( point ##wise mutual information ) out ##per ##forms a hmm based system and two linguistic ##ally motivated systems : lex ##sta ##t and ali ##ne . our results suggest that a pm ##i system trained in an online fashion can be used by historical linguist ##s for fast and accurate identification of co ##gna ##tes in not so well - studied language families . [SEP],[CLS] natasha : faster non - convex st ##och ##astic optimization via strongly non - convex parameter given a non ##con ##ve ##x function that is an average of $ n $ smooth functions , we design st ##och ##astic first - order methods to find its approximate stationary points . the convergence of our new methods depends on the smallest ( negative ) e ##igen ##val ##ue $ - \\ sigma $ of the hess ##ian , a parameter that describes how non ##con ##ve ##x the function is . our methods out ##per ##form known results for a range of parameter $ \\ sigma $ , and can be used to find approximate local mini ##ma . our result implies an interesting di ##cho ##tom ##y : there exists a threshold $ \\ sigma _ 0 $ so that the currently fastest methods for $ \\ sigma > \\ sigma _ 0 $ and for $ \\ sigma < \\ sigma _ 0 $ have different behaviors : the former scales with $ n ^ { 2 / 3 } $ and the latter scales with $ n ^ { 3 / 4 } $ . [SEP],[CLS] ku ##sta ##an ##heim ##o - st ##ie ##fe ##l transformation with an arbitrary defining vector ku ##sta ##an ##heim ##o - st ##ie ##fe ##l ( ks ) transformation depends on the choice of some preferred direction in the cart ##esian 3d space . this choice , seldom explicitly mentioned , amounts typically to the direction of the first or the third coordinate axis in celestial mechanics and atomic physics , respectively . the present work develops a canonical ks transformation with an arbitrary preferred direction , indicated by what we call a defining vector . using a mix of vector and qu ##ater ##nio ##n algebra , we formula ##te the transformation in a reference frame independent manner . the link between the os ##ci ##lla ##tor and kepler ##ian first integral ##s is given . as an example of the present formulation , the kepler ##ian motion in a rotating frame is re - investigated . [SEP]\n",
              "y: MultiCategoryList\n",
              "Computer Science;Physics,Computer Science;Statistics,Computer Science,Computer Science;Mathematics;Statistics,Mathematics\n",
              "Path: .;\n",
              "\n",
              "Valid: LabelList (2098 items)\n",
              "x: TextList\n",
              "[CLS] dynamic layer normal ##ization for adaptive neural acoustic modeling in speech recognition layer normal ##ization is a recently introduced technique for normal ##izing the activities of neurons in deep neural networks to improve the training speed and stability . in this paper , we introduce a new layer normal ##ization technique called dynamic layer normal ##ization ( dl ##n ) for adaptive neural acoustic modeling in speech recognition . by dynamic ##ally generating the scaling and shifting parameters in layer normal ##ization , dl ##n adapt ##s neural acoustic models to the acoustic variability arising from various factors such as speakers , channel noises , and environments . unlike other adaptive acoustic models , our proposed approach does not require additional adaptation data or speaker information such as i - vectors . moreover , the model size is fixed as it dynamic ##ally generates adaptation parameters . we apply our proposed dl ##n to deep bid ##ire ##ction ##al l ##st ##m acoustic models and evaluate them on two bench ##mark data ##set ##s for large vocabulary as ##r experiments : w ##s ##j and ted - liu ##m release 2 . the experimental results show that our dl ##n improves neural acoustic models in terms of transcription accuracy by dynamic ##ally adapting to various speakers and environments . [SEP],[CLS] su ##sc ##ept ##ibility propagation by using diagonal consistency a su ##sc ##ept ##ibility propagation that is constructed by combining a belief propagation and a linear response method is used for approximate computation for marko ##v random fields . here ##in , we formula ##te a new , improved su ##sc ##ept ##ibility propagation by using the concept of a diagonal matching method that is based on mean - field approaches to inverse is ##ing problems . the proposed su ##sc ##ept ##ibility propagation is robust for various network structures , and it is reduced to the ordinary su ##sc ##ept ##ibility propagation and to the adaptive thou ##less - anderson - palmer equation in special cases . [SEP],[CLS] the robot routing problem for collecting aggregate st ##och ##astic rewards we propose a new model for formal ##izing reward collection problems on graphs with dynamic ##ally generated rewards which may appear and disappear based on a st ##och ##astic model . the * robot routing problem * is modeled as a graph whose nodes are st ##och ##astic processes generating potential rewards over discrete time . the rewards are generated according to the st ##och ##astic process , but at each step , an existing reward disappears with a given probability . the edges in the graph en ##code the ( unit - distance ) paths between the rewards ' locations . on visiting a node , the robot collects the accumulated reward at the node at that time , but traveling between the nodes takes time . the optimization question asks to compute an optimal ( or epsilon - optimal ) path that maximize ##s the expected collected rewards . we consider the finite and infinite - horizon robot routing problems . for finite - horizon , the goal is to maximize the total expected reward , while for infinite horizon we consider limit - average objectives . we study the computational and strategy complexity of these problems , establish np - lower bounds and show that optimal strategies require memory in general . we also provide an algorithm for computing epsilon - optimal infinite paths for arbitrary epsilon > 0 . [SEP],[CLS] probability , statistics and planet earth , i : geo ##tem ##por ##al co ##var ##iance ##s the study of co ##var ##iance ##s ( or positive definite functions ) on the sphere ( the earth , in our motivation ) goes back to bo ##chner and sc ##hoe ##nberg ( 1940 - - 42 ) and to the first author ( 1969 , 1973 ) , among others . extending to the geo ##tem ##por ##al case ( sphere cross line , for position and time ) was for a long time an obstacle to geo ##sta ##tist ##ical modelling . the character ##isation question here was raised by the authors and mi ##ja ##tov ##ic in 2016 , and answered by berg and por ##cu in 2017 . extensions to multiple products ( of spheres and lines ) follows similarly ( gu ##ella , men ##ega ##tto and per ##on , 2016 ) . we survey results of this type , and related applications e . g . in numerical weather prediction . [SEP],[CLS] counting the number of meta ##sta ##ble states in the modular ##ity landscape : algorithm ##ic detect ##ability limit of greedy algorithms in community detection modular ##ity maxim ##ization using greedy algorithms continues to be a popular approach toward community detection in graphs , even after various better forming algorithms have been proposed . apart from its clear mechanism and ease of implementation , this approach is persistent ##ly popular because , presumably , its risk of algorithm ##ic failure is not well understood . this rapid communication provides insight into this issue by est ##imating the algorithm ##ic performance limit of modular ##ity maxim ##ization . this is achieved by counting the number of meta ##sta ##ble states under a local update rule . our results offer a quantitative insight into the level of spa ##rs ##ity at which a greedy algorithm typically fails . [SEP]\n",
              "y: MultiCategoryList\n",
              "Computer Science,Mathematics;Statistics,Computer Science;Mathematics,Physics;Statistics,Computer Science\n",
              "Path: .;\n",
              "\n",
              "Test: None, model=BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=BCEWithLogitsLoss(), metrics=[functools.partial(<function accuracy_thresh at 0x7f59122ffd90>, thresh=0.2), functools.partial(<function fbeta at 0x7f59122ff9d8>, thresh=0.2, beta=1)], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='/content/drive/My Drive/Analytics Vidhya/models/bert', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[Sequential(\n",
              "  (0): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): BertLayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (1): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (2): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (3): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (1): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (2): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (1): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (2): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): Dropout(p=0.1, inplace=False)\n",
              "  (1): Linear(in_features=768, out_features=6, bias=True)\n",
              ")], add_time=True, silent=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s6FlRSKbZBn6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "077cbf91-f624-4e97-c358-ea2b823d9183"
      },
      "source": [
        "learner.freeze_to(-2)\n",
        "learner.fit_one_cycle(3, max_lr=slice(1e-4/(2.6**4), 1e-4), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy_thresh</th>\n",
              "      <th>fbeta</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.126350</td>\n",
              "      <td>0.172408</td>\n",
              "      <td>0.917382</td>\n",
              "      <td>0.846981</td>\n",
              "      <td>05:47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.117665</td>\n",
              "      <td>0.176549</td>\n",
              "      <td>0.921115</td>\n",
              "      <td>0.851065</td>\n",
              "      <td>05:57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.099948</td>\n",
              "      <td>0.179491</td>\n",
              "      <td>0.921671</td>\n",
              "      <td>0.849889</td>\n",
              "      <td>05:46</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kImJ36_NZoac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "df671249-c81e-428c-befd-86cf08efba9e"
      },
      "source": [
        "learner.save('head-2')\n",
        "learner.load('head-2')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Learner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (18874 items)\n",
              "x: TextList\n",
              "[CLS] cluster ##ing in hilbert space of a quantum optimization problem the solution space of many classical optimization problems breaks up into clusters which are extensively distant from one another in the ham ##ming metric . here , we show that an analogous quantum cluster ##ing phenomenon takes place in the ground state subsp ##ace of a certain quantum optimization problem . this involves extending the notion of cluster ##ing to hilbert space , where the classical ham ##ming distance is not immediately useful . quantum clusters correspond to macro ##scopic ##ally distinct subsp ##ace ##s of the full quantum ground state space which grow with the system size . we explicitly demonstrate that such clusters arise in the solution space of random quantum sat ##is ##fia ##bility ( 3 - q ##sat ) at its sat ##is ##fia ##bility transition . we estimate both the number of these clusters and their internal entropy . the former are given by the number of hardcore dime ##r covering ##s of the core of the interaction graph , while the latter is related to the under ##con ##stra ##ined degrees of freedom not touched by the dime ##rs . we additionally provide new numerical evidence suggesting that the 3 - q ##sat sat ##is ##fia ##bility transition may coincide with the product sat ##is ##fia ##bility transition , which would imply the absence of an intermediate en ##tangled sat ##is ##fia ##ble phase . [SEP],[CLS] graph heat mixture model learning graph inference methods have recently attracted a great interest from the scientific community , due to the large value they bring in data interpretation and analysis . however , most of the available state - of - the - art methods focus on scenarios where all available data can be explained through the same graph , or groups corresponding to each graph are known a prior ##i . in this paper , we argue that this is not always realistic and we introduce a genera ##tive model for mixed signals following a heat diffusion process on multiple graphs . we propose an expectation - maxim ##isation algorithm that can successfully separate signals into corresponding groups , and in ##fer multiple graphs that govern their behaviour . we demonstrate the benefits of our method on both synthetic and real data . [SEP],[CLS] fast and un ##su ##per ##vis ##ed methods for multi ##ling ##ual co ##gna ##te cluster ##ing in this paper we explore the use of un ##su ##per ##vis ##ed methods for detecting co ##gna ##tes in multi ##ling ##ual word lists . we use online em to train sound segment similarity weights for computing similarity between two words . we tested our online systems on geographically spread sixteen different language groups of the world and show that the online pm ##i system ( point ##wise mutual information ) out ##per ##forms a hmm based system and two linguistic ##ally motivated systems : lex ##sta ##t and ali ##ne . our results suggest that a pm ##i system trained in an online fashion can be used by historical linguist ##s for fast and accurate identification of co ##gna ##tes in not so well - studied language families . [SEP],[CLS] natasha : faster non - convex st ##och ##astic optimization via strongly non - convex parameter given a non ##con ##ve ##x function that is an average of $ n $ smooth functions , we design st ##och ##astic first - order methods to find its approximate stationary points . the convergence of our new methods depends on the smallest ( negative ) e ##igen ##val ##ue $ - \\ sigma $ of the hess ##ian , a parameter that describes how non ##con ##ve ##x the function is . our methods out ##per ##form known results for a range of parameter $ \\ sigma $ , and can be used to find approximate local mini ##ma . our result implies an interesting di ##cho ##tom ##y : there exists a threshold $ \\ sigma _ 0 $ so that the currently fastest methods for $ \\ sigma > \\ sigma _ 0 $ and for $ \\ sigma < \\ sigma _ 0 $ have different behaviors : the former scales with $ n ^ { 2 / 3 } $ and the latter scales with $ n ^ { 3 / 4 } $ . [SEP],[CLS] ku ##sta ##an ##heim ##o - st ##ie ##fe ##l transformation with an arbitrary defining vector ku ##sta ##an ##heim ##o - st ##ie ##fe ##l ( ks ) transformation depends on the choice of some preferred direction in the cart ##esian 3d space . this choice , seldom explicitly mentioned , amounts typically to the direction of the first or the third coordinate axis in celestial mechanics and atomic physics , respectively . the present work develops a canonical ks transformation with an arbitrary preferred direction , indicated by what we call a defining vector . using a mix of vector and qu ##ater ##nio ##n algebra , we formula ##te the transformation in a reference frame independent manner . the link between the os ##ci ##lla ##tor and kepler ##ian first integral ##s is given . as an example of the present formulation , the kepler ##ian motion in a rotating frame is re - investigated . [SEP]\n",
              "y: MultiCategoryList\n",
              "Computer Science;Physics,Computer Science;Statistics,Computer Science,Computer Science;Mathematics;Statistics,Mathematics\n",
              "Path: .;\n",
              "\n",
              "Valid: LabelList (2098 items)\n",
              "x: TextList\n",
              "[CLS] dynamic layer normal ##ization for adaptive neural acoustic modeling in speech recognition layer normal ##ization is a recently introduced technique for normal ##izing the activities of neurons in deep neural networks to improve the training speed and stability . in this paper , we introduce a new layer normal ##ization technique called dynamic layer normal ##ization ( dl ##n ) for adaptive neural acoustic modeling in speech recognition . by dynamic ##ally generating the scaling and shifting parameters in layer normal ##ization , dl ##n adapt ##s neural acoustic models to the acoustic variability arising from various factors such as speakers , channel noises , and environments . unlike other adaptive acoustic models , our proposed approach does not require additional adaptation data or speaker information such as i - vectors . moreover , the model size is fixed as it dynamic ##ally generates adaptation parameters . we apply our proposed dl ##n to deep bid ##ire ##ction ##al l ##st ##m acoustic models and evaluate them on two bench ##mark data ##set ##s for large vocabulary as ##r experiments : w ##s ##j and ted - liu ##m release 2 . the experimental results show that our dl ##n improves neural acoustic models in terms of transcription accuracy by dynamic ##ally adapting to various speakers and environments . [SEP],[CLS] su ##sc ##ept ##ibility propagation by using diagonal consistency a su ##sc ##ept ##ibility propagation that is constructed by combining a belief propagation and a linear response method is used for approximate computation for marko ##v random fields . here ##in , we formula ##te a new , improved su ##sc ##ept ##ibility propagation by using the concept of a diagonal matching method that is based on mean - field approaches to inverse is ##ing problems . the proposed su ##sc ##ept ##ibility propagation is robust for various network structures , and it is reduced to the ordinary su ##sc ##ept ##ibility propagation and to the adaptive thou ##less - anderson - palmer equation in special cases . [SEP],[CLS] the robot routing problem for collecting aggregate st ##och ##astic rewards we propose a new model for formal ##izing reward collection problems on graphs with dynamic ##ally generated rewards which may appear and disappear based on a st ##och ##astic model . the * robot routing problem * is modeled as a graph whose nodes are st ##och ##astic processes generating potential rewards over discrete time . the rewards are generated according to the st ##och ##astic process , but at each step , an existing reward disappears with a given probability . the edges in the graph en ##code the ( unit - distance ) paths between the rewards ' locations . on visiting a node , the robot collects the accumulated reward at the node at that time , but traveling between the nodes takes time . the optimization question asks to compute an optimal ( or epsilon - optimal ) path that maximize ##s the expected collected rewards . we consider the finite and infinite - horizon robot routing problems . for finite - horizon , the goal is to maximize the total expected reward , while for infinite horizon we consider limit - average objectives . we study the computational and strategy complexity of these problems , establish np - lower bounds and show that optimal strategies require memory in general . we also provide an algorithm for computing epsilon - optimal infinite paths for arbitrary epsilon > 0 . [SEP],[CLS] probability , statistics and planet earth , i : geo ##tem ##por ##al co ##var ##iance ##s the study of co ##var ##iance ##s ( or positive definite functions ) on the sphere ( the earth , in our motivation ) goes back to bo ##chner and sc ##hoe ##nberg ( 1940 - - 42 ) and to the first author ( 1969 , 1973 ) , among others . extending to the geo ##tem ##por ##al case ( sphere cross line , for position and time ) was for a long time an obstacle to geo ##sta ##tist ##ical modelling . the character ##isation question here was raised by the authors and mi ##ja ##tov ##ic in 2016 , and answered by berg and por ##cu in 2017 . extensions to multiple products ( of spheres and lines ) follows similarly ( gu ##ella , men ##ega ##tto and per ##on , 2016 ) . we survey results of this type , and related applications e . g . in numerical weather prediction . [SEP],[CLS] counting the number of meta ##sta ##ble states in the modular ##ity landscape : algorithm ##ic detect ##ability limit of greedy algorithms in community detection modular ##ity maxim ##ization using greedy algorithms continues to be a popular approach toward community detection in graphs , even after various better forming algorithms have been proposed . apart from its clear mechanism and ease of implementation , this approach is persistent ##ly popular because , presumably , its risk of algorithm ##ic failure is not well understood . this rapid communication provides insight into this issue by est ##imating the algorithm ##ic performance limit of modular ##ity maxim ##ization . this is achieved by counting the number of meta ##sta ##ble states under a local update rule . our results offer a quantitative insight into the level of spa ##rs ##ity at which a greedy algorithm typically fails . [SEP]\n",
              "y: MultiCategoryList\n",
              "Computer Science,Mathematics;Statistics,Computer Science;Mathematics,Physics;Statistics,Computer Science\n",
              "Path: .;\n",
              "\n",
              "Test: None, model=BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=BCEWithLogitsLoss(), metrics=[functools.partial(<function accuracy_thresh at 0x7f59122ffd90>, thresh=0.2), functools.partial(<function fbeta at 0x7f59122ff9d8>, thresh=0.2, beta=1)], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='/content/drive/My Drive/Analytics Vidhya/models/bert', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[Sequential(\n",
              "  (0): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): BertLayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (1): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (2): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (3): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (1): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (2): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (1): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (2): BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSelfAttention(\n",
              "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (LayerNorm): BertLayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): Dropout(p=0.1, inplace=False)\n",
              "  (1): Linear(in_features=768, out_features=6, bias=True)\n",
              ")], add_time=True, silent=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qo4V3sNfZufc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "f8f12d16-6efe-4f27-847c-bb0ebce1e941"
      },
      "source": [
        "learner.unfreeze()\n",
        "learner.lr_find()\n",
        "learner.recorder.plot(suggestion=True)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.00% [0/1 00:00<00:00]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy_thresh</th>\n",
              "      <th>fbeta</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='65' class='' max='1179' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      5.51% [65/1179 00:26<07:34 0.3599]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
            "Min numerical gradient: 1.91E-06\n",
            "Min loss divided by 10: 3.31E-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S3XuE-ZcZuii",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "44938585-3cd8-4be7-d568-f1417732d102"
      },
      "source": [
        "learner.fit_one_cycle(2, slice(3e-6/(2.6**4), 3e-6), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy_thresh</th>\n",
              "      <th>fbeta</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.101703</td>\n",
              "      <td>0.179716</td>\n",
              "      <td>0.921115</td>\n",
              "      <td>0.848999</td>\n",
              "      <td>08:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.109944</td>\n",
              "      <td>0.179969</td>\n",
              "      <td>0.921354</td>\n",
              "      <td>0.849396</td>\n",
              "      <td>08:38</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YUATIviOp7T3",
        "colab": {}
      },
      "source": [
        "learner.save('final')"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F32A2vz4iEJd",
        "colab": {}
      },
      "source": [
        "test_datalist = TextList.from_df(test_df, cols='text',\n",
        "                  vocab=fastai_bert_vocab\n",
        "                  )"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ivbAgEyAm6zr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "08d10076-2ef4-432d-98b4-c65beb0c0c03"
      },
      "source": [
        "databunch_1.add_test(test_datalist)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i5J-pY-bqFpg"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_5HbX9YEqBmL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b1f2a64f-1dd3-42da-f764-a2c7f208f037"
      },
      "source": [
        "preds, target = learner.get_preds(DatasetType.Test)\n",
        "labels = preds.numpy()\n",
        "\n",
        "submission = pd.DataFrame({'ID': test_id})\n",
        "submission = pd.concat([submission, pd.DataFrame(preds.numpy(), columns = label_cols)], axis=1)\n",
        "\n",
        "submission.to_csv('/content/drive/My Drive/Analytics Vidhya/output/bert_proba.csv', index=False)\n",
        "submission.head()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Computer Science</th>\n",
              "      <th>Physics</th>\n",
              "      <th>Mathematics</th>\n",
              "      <th>Statistics</th>\n",
              "      <th>Quantitative Biology</th>\n",
              "      <th>Quantitative Finance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20973</td>\n",
              "      <td>0.358784</td>\n",
              "      <td>0.001945</td>\n",
              "      <td>0.374500</td>\n",
              "      <td>0.971995</td>\n",
              "      <td>0.000565</td>\n",
              "      <td>4.416108e-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20974</td>\n",
              "      <td>0.001859</td>\n",
              "      <td>0.998249</td>\n",
              "      <td>0.002674</td>\n",
              "      <td>0.001423</td>\n",
              "      <td>0.000135</td>\n",
              "      <td>6.696432e-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20975</td>\n",
              "      <td>0.995202</td>\n",
              "      <td>0.002088</td>\n",
              "      <td>0.020677</td>\n",
              "      <td>0.007275</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>9.835653e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20976</td>\n",
              "      <td>0.001720</td>\n",
              "      <td>0.998258</td>\n",
              "      <td>0.002757</td>\n",
              "      <td>0.001583</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>7.295190e-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20977</td>\n",
              "      <td>0.994843</td>\n",
              "      <td>0.006588</td>\n",
              "      <td>0.107775</td>\n",
              "      <td>0.002634</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>2.928813e-06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      ID  Computer Science  ...  Quantitative Biology  Quantitative Finance\n",
              "0  20973          0.358784  ...              0.000565          4.416108e-04\n",
              "1  20974          0.001859  ...              0.000135          6.696432e-06\n",
              "2  20975          0.995202  ...              0.000018          9.835653e-07\n",
              "3  20976          0.001720  ...              0.000155          7.295190e-06\n",
              "4  20977          0.994843  ...              0.000032          2.928813e-06\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    }
  ]
}